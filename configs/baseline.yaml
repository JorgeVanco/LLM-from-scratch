# Baseline configuration for small-scale experiments

experiment_name: "baseline"
description: "Baseline small model for testing"
seed: 42

model:
    vocab_size: 10000
    context_length: 256
    num_layers: 4
    d_model: 512
    num_heads: 16
    d_ff: 1344
    rope_theta: 10000.0
    post_norm: False
    ffn_type: swiglu

optimizer:
    name: "Muon"
    lr: 4e-3
    betas: [0.8, 0.95]
    eps: 1e-8
    weight_decay: 0.1
    muon_lr: 0.05
    muon_momentum: 0.95

scheduler:
    use_scheduler: true
    name: "warmup_stable_decay"
    use_multiplier: true
    warmup_frac: 0.0
    decay_frac: 0.2
    max_learning_rate: 4e-3
    min_learning_rate: 3e-5
    warmup_iters: 1000
    cosine_cycle_iters: 20000
    cosine_cycle_frac: 0.8

training:
    batch_size: 256
    max_iters: 5000
    max_tokens: null
    eval_interval: 100
    eval_iters: 100
    log_interval: 50
    save_interval: 2500
    gradient_clip_val: 1.0
    compile_model: true
    train_data_path: "data/tokenized/TinyStoriesV2-GPT4/10000/train.npy"
    val_data_path: "data/tokenized/TinyStoriesV2-GPT4/10000/val.npy"
    device: "auto"
    dtype: "float32"

tokenizer:
    tokenizer_path: "tokenizer/tiny-stories/10000"
    special_tokens: ["<|endoftext|>"]

logging:
    project_name: "llm-from-scratch"
    run_name: "baseline"
    log_dir: "logs"
    checkpoint_dir: "checkpoints"
    use_wandb: true
    wandb_project: "llm-from-scratch"
    tags: []